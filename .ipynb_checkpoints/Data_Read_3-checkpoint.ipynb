{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "339c18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as num\n",
    "import matplotlib.pyplot as plt\n",
    "import gurobipy as gp\n",
    "from   gurobipy import GRB\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import warnings\n",
    "import re\n",
    "import collections\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import Data_Functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b5252a",
   "metadata": {},
   "source": [
    "# Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "655459a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'C:/Users/bount/Colonial-Storage/data_new_3/'\n",
    "filenames = os.listdir(directory)\n",
    "cd = os.getcwd()\n",
    "\n",
    "dfs = []\n",
    "for file in filenames:\n",
    "    df = pd.read_excel (directory + file, header=0, sheet_name = \"Sheet1\")\n",
    "    df['file']  = file\n",
    "    df['Start'] = pd.to_datetime(df['Start Date/Time'])\n",
    "    df['End']   = pd.to_datetime(df['End Date/Time'])\n",
    "    df['Identifier'] = df['Code'] + '//' + df['Start'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S')) + '//' + df['End'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S')) + '//' + df['Line'].astype(str)\n",
    "    dfs.append(df)\n",
    "\n",
    "df_1 = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba47dc9",
   "metadata": {},
   "source": [
    "# Add Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbbcb5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "# Add Client, Product, Cycle\n",
    "#\n",
    "temp = df_1['Code'].str.extract(r'(\\w+)-(\\w+)-(\\w+)')\n",
    "temp.columns = ['Client', 'Product', 'Cycle']\n",
    "df_1 = pd.concat([df_1, temp], axis=1)\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# Add Product\n",
    "#\n",
    "def product(df):\n",
    "    if (df['Product Grade'] in ['A3', 'A4']):\n",
    "        return 'A'\n",
    "    elif (df['Product Grade'] in ['D3', 'D4']):\n",
    "        return 'D'\n",
    "    else:\n",
    "        return df['Product Grade']\n",
    "df_1['Product'] = df_1.apply(product, axis = 1)\n",
    "df_1['Product'] = df_1['Product'].astype(str)\n",
    "    \n",
    "#-------------------------------------------------------------------------\n",
    "# Add Line\n",
    "#    \n",
    "def line(df):\n",
    "    if (df['Line'] in [1]):\n",
    "        return '01'\n",
    "    if (df['Line'] in [2]):\n",
    "        return '02'\n",
    "    else:\n",
    "        return df['Line']\n",
    "df_1['Line']    = df_1.apply(line, axis = 1)\n",
    "df_1['Line']    = df_1['Line'].astype(str)\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# Add Type\n",
    "#\n",
    "def typ(df):\n",
    "    if (df['Line'] in ['01', '02']):\n",
    "        return 'In'\n",
    "    else:\n",
    "        return 'Out'\n",
    "df_1['Type']    = df_1.apply(typ, axis = 1)\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# Add Exclude\n",
    "#\n",
    "df_1['Time_difference'] = df_1['End'] - df_1['Start']\n",
    "df_1['Time_in_hours']   = df_1['Time_difference'] / pd.Timedelta(hours=1)\n",
    "df_1['Vol_per_Hr']      = df_1['Volume'] / df_1['Time_in_hours']\n",
    "def exclude(df):\n",
    "    if (df['Vol_per_Hr'] >= 50):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    \n",
    "df_1['Exclude'] = df_1.apply(exclude, axis = 1)\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# Misc\n",
    "#\n",
    "df_1 = df_1.rename(columns={'Vol Tank':'Volume_Tank'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7c8534",
   "metadata": {},
   "source": [
    "# Collapse Cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2efe09c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.loc[df_1['Cycle'] == '02A', 'Cycle'] = '021'\n",
    "df_1.loc[df_1['Cycle'] == '022', 'Cycle'] = '021'\n",
    "df_1.loc[df_1['Cycle'] == '023', 'Cycle'] = '021'\n",
    "df_1.loc[df_1['Cycle'] == '032', 'Cycle'] = '031'\n",
    "df_1.loc[df_1['Cycle'] == '035', 'Cycle'] = '031'\n",
    "df_1.loc[df_1['Cycle'] == '03A', 'Cycle'] = '031'\n",
    "df_1.loc[df_1['Cycle'] == '042', 'Cycle'] = '041'\n",
    "df_1.loc[df_1['Cycle'] == '043', 'Cycle'] = '041'\n",
    "df_1.loc[df_1['Cycle'] == '045', 'Cycle'] = '041'\n",
    "df_1.loc[df_1['Cycle'] == '046', 'Cycle'] = '041'\n",
    "df_1.loc[df_1['Cycle'] == '04A', 'Cycle'] = '041'\n",
    "df_1.loc[df_1['Cycle'] == '04B', 'Cycle'] = '041'\n",
    "df_1.loc[df_1['Cycle'] == '04K', 'Cycle'] = '041'\n",
    "df_1.loc[df_1['Cycle'] == '052', 'Cycle'] = '051'\n",
    "df_1.loc[df_1['Cycle'] == '055', 'Cycle'] = '051'\n",
    "df_1.loc[df_1['Cycle'] == '05A', 'Cycle'] = '051'\n",
    "df_1.loc[df_1['Cycle'] == '062', 'Cycle'] = '061'\n",
    "df_1.loc[df_1['Cycle'] == '063', 'Cycle'] = '061'\n",
    "df_1.loc[df_1['Cycle'] == '06A', 'Cycle'] = '061'\n",
    "df_1.loc[df_1['Cycle'] == '06B', 'Cycle'] = '061'\n",
    "df_1.loc[df_1['Cycle'] == '072', 'Cycle'] = '071'\n",
    "df_1.loc[df_1['Cycle'] == '074', 'Cycle'] = '071'\n",
    "df_1.loc[df_1['Cycle'] == '082', 'Cycle'] = '081'\n",
    "df_1.loc[df_1['Cycle'] == '084', 'Cycle'] = '081'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1129c04",
   "metadata": {},
   "source": [
    "# Exclude Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e50b28fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are excluding 36 rows with errors.\n",
      "We are excluding 81 rows with other products.\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "# Remove rows\n",
    "#\n",
    "nan_values = df_1.isna()\n",
    "index      = df_1.loc[nan_values['Tank'] == True].index\n",
    "df_1       = df_1.drop(index=index)\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# Exclude products not in A, D, 54, 62\n",
    "#\n",
    "df_1 = df_1[(df_1['Cycle'].isin(['021', '031', '041', '051', '061', '071', '081']))]\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# Exclude columns with errors\n",
    "# \n",
    "print('We are excluding ' + str(sum(df_1['Exclude'])) + ' rows with errors.')\n",
    "df_1 = df_1[df_1['Exclude'] == 0]\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# Exclude products not in A, D, 54, 62\n",
    "#\n",
    "print('We are excluding ' + str(sum(~(df_1['Product'].isin(['A', 'D', '54', '62'])))) + ' rows with other products.')\n",
    "df_1 = df_1[(df_1['Product'].isin(['A', 'D', '54', '62']))]\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "# Remove rows due to Line/Product combinations\n",
    "#\n",
    "df_1['Tank'] = df_1['Tank'].astype(int)\n",
    "df_1 = df_1[~((df_1['Line'].isin(['20']) & df_1['Product'].isin(['A'])))]\n",
    "df_1 = df_1[~((df_1['Line'].isin(['20']) & df_1['Product'].isin(['D'])))]\n",
    "df_1 = df_1[~((df_1['Tank'].isin([310]) & df_1['Product'].isin(['D'])))]\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "# When Volume_Tank >= 900 replace it with 10 - it is a data error\n",
    "#\n",
    "df_1.loc[df_1['Volume_Tank'] >= 900, 'Volume_Tank'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ce33c",
   "metadata": {},
   "source": [
    "# Create the tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c564f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  file Cycle  Volume_v2  Products  Count\n",
      "2  ATJ.2023.01.18.xlsx   031    182.995         4      2\n",
      "3  ATJ.2023.01.20.xlsx   031    183.765         4      2\n",
      "4  ATJ.2023.01.23.xlsx   031     -5.000         4      2\n",
      "                  file Cycle  Volume_v2  Products  Count\n",
      "5  ATJ.2023.01.23.xlsx   041    380.827         4      2\n",
      "6  ATJ.2023.01.25.xlsx   041    237.645         4      2\n",
      "7  ATJ.2023.01.27.xlsx   041    -23.955         4      2\n",
      "                   file Cycle  Volume_v2  Products  Count\n",
      "8   ATJ.2023.01.27.xlsx   051    175.774         4      2\n",
      "9   ATJ.2023.01.30.xlsx   051    442.388         4      2\n",
      "10  ATJ.2023.02.01.xlsx   051    176.513         4      2\n",
      "11  ATJ.2023.02.03.xlsx   051    155.999         4      2\n",
      "                   file Cycle  Volume_v2  Products  Count\n",
      "12  ATJ.2023.02.01.xlsx   061    269.880         4      2\n",
      "13  ATJ.2023.02.03.xlsx   061    339.461         4      2\n",
      "14  ATJ.2023.02.06.xlsx   061     83.000         4      2\n",
      "                   file Cycle  Volume_v2  Products  Count\n",
      "15  ATJ.2023.02.06.xlsx   071    151.800         4      2\n",
      "16  ATJ.2023.02.08.xlsx   071    136.452         4      2\n",
      "17  ATJ.2023.02.10.xlsx   071   -165.828         4      2\n",
      "18  ATJ.2023.02.13.xlsx   071   -219.499         4      2\n",
      "                   file Cycle  Volume_v2  Products  Count\n",
      "19  ATJ.2023.02.13.xlsx   081    293.540         4      2\n",
      "20  ATJ.2023.02.15.xlsx   081    175.800         4      2\n",
      "21  ATJ.2023.02.17.xlsx   081    176.659         4      2\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "# Create the \"Pick\" column\n",
    "# \n",
    "a = df_1[['file', 'Cycle', 'Line', 'Type', 'Volume', 'Tank', 'Product']]\n",
    "a = a.loc[a[\"Product\"] == '62']\n",
    "a = a.groupby(['file', 'Cycle', 'Type']).agg({'Volume': ['sum']}).reset_index()\n",
    "a.columns = a.columns.droplevel(level=1)\n",
    "def typ(df):\n",
    "    if (df['Type'] in ['In']):\n",
    "        return df['Volume']\n",
    "    else:\n",
    "        return -df['Volume']\n",
    "a['Volume_v2']    = a.apply(typ, axis = 1)\n",
    "a = a.groupby(['file', 'Cycle']).agg({'Volume_v2': ['sum']}).reset_index()\n",
    "a.columns = a.columns.droplevel(level=1)\n",
    "a = a.sort_values(by=['Cycle', 'file'], ascending=[True, True])\n",
    "\n",
    "b = df_1.groupby(['file', 'Cycle', 'Type'])['Product'].nunique()\n",
    "b = pd.DataFrame(b)\n",
    "b = b.reset_index()\n",
    "b = b.groupby(['file', 'Cycle']).agg({'Product': ['min', 'count']}).reset_index()\n",
    "b.columns = b.columns.droplevel(level=1)\n",
    "b.columns = ['file', 'Cycle', 'Products', 'Count']\n",
    "b = b.loc[(b['Products'] == 4) & (b['Count'] == 2)]\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# Selection\n",
    "#\n",
    "a = pd.merge(a, b, on = ['file', 'Cycle'])\n",
    "\n",
    "print(a.loc[(a['Cycle'] == '031')]) #ATJ.2023.01.18.xlsx\n",
    "print(a.loc[(a['Cycle'] == '041')]) #ATJ.2023.01.25.xlsx\n",
    "print(a.loc[(a['Cycle'] == '051')]) #ATJ.2023.01.30.xlsx\n",
    "print(a.loc[(a['Cycle'] == '061')]) #ATJ.2023.02.06.xlsx\n",
    "print(a.loc[(a['Cycle'] == '071')]) #ATJ.2023.02.10.xlsx\n",
    "print(a.loc[(a['Cycle'] == '081')]) #ATJ.2023.02.13.xlsx\n",
    "\n",
    "a = pd.DataFrame({\n",
    "'file':['ATJ.2023.01.18.xlsx', 'ATJ.2023.01.25.xlsx', 'ATJ.2023.01.30.xlsx', 'ATJ.2023.02.06.xlsx', 'ATJ.2023.02.10.xlsx', 'ATJ.2023.02.13.xlsx'],\n",
    "'Cycle':['031', '041', '051', '061', '071', '081'],\n",
    "'Pick':[1,1,1,1,1,1]\n",
    "})\n",
    "\n",
    "df_3       = pd.merge(df_1, a, on = ['Cycle', 'file'], how='outer')\n",
    "df_4       = df_3[df_3['Pick'] == 1]\n",
    "df_tickets = df_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718a2db",
   "metadata": {},
   "source": [
    "# Auditing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "038f73be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bounds   = {\n",
    "            \"01\": {\"l\": 25,   \"u\": 28},\n",
    "            \"02\": {\"l\": 6,    \"u\": 10},\n",
    "            \"13\": {\"l\": 2,    \"u\": 2},\n",
    "            \"14\": {\"l\": 2,    \"u\": 2.4}, \n",
    "            \"15\": {\"l\": 4,    \"u\": 5},\n",
    "            \"16\": {\"l\": 2,    \"u\": 2.15},\n",
    "            \"17\": {\"l\": 4,    \"u\": 5},\n",
    "            \"18\": {\"l\": 7,    \"u\": 8.7},\n",
    "            \"19\": {\"l\": 4,    \"u\": 5.1},\n",
    "            \"20\": {\"l\": 1,    \"u\": 1.75},\n",
    "            \"1A\": {\"l\": 1,    \"u\": 5},\n",
    "            \"2A\": {\"l\": 1,    \"u\": 5}\n",
    "           }\n",
    "\n",
    "a = df_tickets.groupby(['Cycle', 'Line']).agg({'Volume': 'sum'}) \n",
    "a = a.reset_index()\n",
    "ab= pd.DataFrame.from_dict(Bounds, orient='index')\n",
    "ab = ab.reset_index()\n",
    "ab.columns = ['Line', 'l', 'u']\n",
    "\n",
    "a = pd.merge(a, ab, on=['Line'], how = 'outer')\n",
    "a['Hours'] = a['Volume'] / a['u']\n",
    "a = a.sort_values(by=['Cycle', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fef8b0",
   "metadata": {},
   "source": [
    "# Remove rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4924a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Cycle***\n",
      "['031', '041', '051', '061', '071', '081']\n",
      "***Tanks***\n",
      "[310, 311, 312, 313, 314, 316, 317, 330, 331, 332, 333, 334, 336, 337, 338, 339, 350, 351, 352, 353, 354, 360, 361, 370, 371, 373, 374]\n",
      "***Products***\n",
      "['54', '62', 'A', 'D']\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "# Audit and remove values\n",
    "#\n",
    "def audit(df):\n",
    "    print(\"***Cycle***\")\n",
    "    a = list(set(list(df.Cycle)))\n",
    "    print(sorted(a))\n",
    "    print(\"***Tanks***\")\n",
    "    print(sorted(list(set(list(df.Tank)))))\n",
    "    print(\"***Products***\")\n",
    "    print(sorted(list(set(list(df.Product)))))\n",
    "    \n",
    "audit(df_tickets)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c70f810c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import Data_Functions as f\n",
    "ret = f.data_volume(df_tickets)\n",
    "top = f.data_topology(df_tickets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9676cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tickets.to_csv('results/tickets_3.csv', index=False)\n",
    "\n",
    "my_dict = ret['VolIn']\n",
    "with open('results/VolIn_3.csv', 'w', newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for key, value in my_dict.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "my_dict = ret['VolOut']\n",
    "with open('results/VolOut_3.csv', 'w', newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for key, value in my_dict.items():\n",
    "        writer.writerow([key, value])\n",
    "        \n",
    "my_dict = ret['VolExist']\n",
    "with open('results/VolExist_3.csv', 'w', newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for key, value in my_dict.items():\n",
    "        writer.writerow([key, value]) \n",
    "\n",
    "my_dict = top        \n",
    "with open('results/Topology_3.csv', 'w', newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for key, value in my_dict.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "        \n",
    "        \n",
    "my_dict = ret['VolLineFlows']        \n",
    "# Concatenate the dataframes into a single dataframe\n",
    "concatenated_df = pd.concat(my_dict.values(), ignore_index=True)\n",
    "# Write the concatenated dataframe to a CSV file\n",
    "concatenated_df.to_csv('results/VolLineFlows_3.csv', index=False)\n",
    "\n",
    "df_dates = df_tickets.groupby('Cycle')['Start Date/Time'].min()\n",
    "df_dates.to_csv('results/dates_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fec6877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Diff</th>\n",
       "      <th>Cycle</th>\n",
       "      <th>Hourly_Vol</th>\n",
       "      <th>Product</th>\n",
       "      <th>Line</th>\n",
       "      <th>Datetime_Min</th>\n",
       "      <th>Time</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>...</th>\n",
       "      <th>Time</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Diff</th>\n",
       "      <th>Cycle</th>\n",
       "      <th>Hourly_Vol</th>\n",
       "      <th>Product</th>\n",
       "      <th>Line</th>\n",
       "      <th>Datetime_Min</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-22 03:00:00</td>\n",
       "      <td>107.906</td>\n",
       "      <td>7.0</td>\n",
       "      <td>031</td>\n",
       "      <td>15.415</td>\n",
       "      <td>A</td>\n",
       "      <td>01</td>\n",
       "      <td>2023-01-22 02:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-01-26 23:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-15 05:00:00</td>\n",
       "      <td>134.221</td>\n",
       "      <td>8.0</td>\n",
       "      <td>081</td>\n",
       "      <td>16.778</td>\n",
       "      <td>A</td>\n",
       "      <td>01</td>\n",
       "      <td>2023-02-14 15:00:00</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-22 04:00:00</td>\n",
       "      <td>107.906</td>\n",
       "      <td>7.0</td>\n",
       "      <td>031</td>\n",
       "      <td>15.415</td>\n",
       "      <td>A</td>\n",
       "      <td>01</td>\n",
       "      <td>2023-01-22 02:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2023-01-27 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-02-15 06:00:00</td>\n",
       "      <td>134.221</td>\n",
       "      <td>8.0</td>\n",
       "      <td>081</td>\n",
       "      <td>16.778</td>\n",
       "      <td>A</td>\n",
       "      <td>01</td>\n",
       "      <td>2023-02-14 15:00:00</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-22 05:00:00</td>\n",
       "      <td>107.906</td>\n",
       "      <td>7.0</td>\n",
       "      <td>031</td>\n",
       "      <td>15.415</td>\n",
       "      <td>A</td>\n",
       "      <td>01</td>\n",
       "      <td>2023-01-22 02:00:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2023-01-27 01:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2023-02-15 07:00:00</td>\n",
       "      <td>134.221</td>\n",
       "      <td>8.0</td>\n",
       "      <td>081</td>\n",
       "      <td>16.778</td>\n",
       "      <td>A</td>\n",
       "      <td>01</td>\n",
       "      <td>2023-02-14 15:00:00</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-22 06:00:00</td>\n",
       "      <td>107.906</td>\n",
       "      <td>7.0</td>\n",
       "      <td>031</td>\n",
       "      <td>15.415</td>\n",
       "      <td>A</td>\n",
       "      <td>01</td>\n",
       "      <td>2023-01-22 02:00:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2023-01-27 02:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2023-02-15 08:00:00</td>\n",
       "      <td>134.221</td>\n",
       "      <td>8.0</td>\n",
       "      <td>081</td>\n",
       "      <td>16.778</td>\n",
       "      <td>A</td>\n",
       "      <td>01</td>\n",
       "      <td>2023-02-14 15:00:00</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-22 07:00:00</td>\n",
       "      <td>107.906</td>\n",
       "      <td>7.0</td>\n",
       "      <td>031</td>\n",
       "      <td>15.415</td>\n",
       "      <td>A</td>\n",
       "      <td>01</td>\n",
       "      <td>2023-01-22 02:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2023-01-27 03:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2023-02-15 09:00:00</td>\n",
       "      <td>134.221</td>\n",
       "      <td>8.0</td>\n",
       "      <td>081</td>\n",
       "      <td>16.778</td>\n",
       "      <td>A</td>\n",
       "      <td>01</td>\n",
       "      <td>2023-02-14 15:00:00</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-01 14:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-01 15:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-01 16:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-01 17:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>993 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Datetime   Volume  Diff Cycle Hourly_Vol Product  Line  \\\n",
       "0   2023-01-22 03:00:00  107.906   7.0   031     15.415       A    01   \n",
       "1   2023-01-22 04:00:00  107.906   7.0   031     15.415       A    01   \n",
       "2   2023-01-22 05:00:00  107.906   7.0   031     15.415       A    01   \n",
       "3   2023-01-22 06:00:00  107.906   7.0   031     15.415       A    01   \n",
       "4   2023-01-22 07:00:00  107.906   7.0   031     15.415       A    01   \n",
       "..                  ...      ...   ...   ...        ...     ...   ...   \n",
       "988                 NaT      NaN   NaN   NaN        NaN     NaN   NaN   \n",
       "989                 NaT      NaN   NaN   NaN        NaN     NaN   NaN   \n",
       "990                 NaT      NaN   NaN   NaN        NaN     NaN   NaN   \n",
       "991                 NaT      NaN   NaN   NaN        NaN     NaN   NaN   \n",
       "992                 NaT      NaN   NaN   NaN        NaN     NaN   NaN   \n",
       "\n",
       "           Datetime_Min  Time            Datetime  ...  Time  \\\n",
       "0   2023-01-22 02:00:00   1.0 2023-01-26 23:00:00  ...   0.0   \n",
       "1   2023-01-22 02:00:00   2.0 2023-01-27 00:00:00  ...   1.0   \n",
       "2   2023-01-22 02:00:00   3.0 2023-01-27 01:00:00  ...   2.0   \n",
       "3   2023-01-22 02:00:00   4.0 2023-01-27 02:00:00  ...   3.0   \n",
       "4   2023-01-22 02:00:00   5.0 2023-01-27 03:00:00  ...   4.0   \n",
       "..                  ...   ...                 ...  ...   ...   \n",
       "988                 NaT   NaN 2023-02-01 14:00:00  ...   NaN   \n",
       "989                 NaT   NaN 2023-02-01 15:00:00  ...   NaN   \n",
       "990                 NaT   NaN 2023-02-01 16:00:00  ...   NaN   \n",
       "991                 NaT   NaN 2023-02-01 17:00:00  ...   NaN   \n",
       "992                 NaT   NaN                 NaT  ...   NaN   \n",
       "\n",
       "               Datetime   Volume  Diff Cycle Hourly_Vol Product  Line  \\\n",
       "0   2023-02-15 05:00:00  134.221   8.0   081     16.778       A    01   \n",
       "1   2023-02-15 06:00:00  134.221   8.0   081     16.778       A    01   \n",
       "2   2023-02-15 07:00:00  134.221   8.0   081     16.778       A    01   \n",
       "3   2023-02-15 08:00:00  134.221   8.0   081     16.778       A    01   \n",
       "4   2023-02-15 09:00:00  134.221   8.0   081     16.778       A    01   \n",
       "..                  ...      ...   ...   ...        ...     ...   ...   \n",
       "988                 NaT      NaN   NaN   NaN        NaN     NaN   NaN   \n",
       "989                 NaT      NaN   NaN   NaN        NaN     NaN   NaN   \n",
       "990                 NaT      NaN   NaN   NaN        NaN     NaN   NaN   \n",
       "991                 NaT      NaN   NaN   NaN        NaN     NaN   NaN   \n",
       "992                 NaT      NaN   NaN   NaN        NaN     NaN   NaN   \n",
       "\n",
       "           Datetime_Min  Time  \n",
       "0   2023-02-14 15:00:00  14.0  \n",
       "1   2023-02-14 15:00:00  15.0  \n",
       "2   2023-02-14 15:00:00  16.0  \n",
       "3   2023-02-14 15:00:00  17.0  \n",
       "4   2023-02-14 15:00:00  18.0  \n",
       "..                  ...   ...  \n",
       "988                 NaT   NaN  \n",
       "989                 NaT   NaN  \n",
       "990                 NaT   NaN  \n",
       "991                 NaT   NaN  \n",
       "992                 NaT   NaN  \n",
       "\n",
       "[993 rows x 54 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Read data from a CSV file into a DataFrame\n",
    "# df = pd.read_csv('results/lineFlows_3.csv')\n",
    "# max(df[(df['Cycle'] == 31)]['Time'])\n",
    "\n",
    "concatenated_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
